{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, IntegerType, ArrayType, DataType\n",
    "import pandas as pd\n",
    "import os \n",
    "import logging\n",
    "import time\n",
    "spark = SparkSession.builder.master(\"local[4]\") \\\n",
    "    .appName(\"ETL_CNPJ\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "file_log = current_dir + r\"/logs/src.log\"\n",
    "\n",
    "# gerando log\n",
    "logging.basicConfig(level=logging.DEBUG, filename=file_log, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Warnings: Possui uma série de funções e comandos para tratamento de mensagens de avisos e alertas do Python\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE O NOVO NOME DE CADA COLUNA\n",
    "NOVO_NOME_COLUNAS = {\n",
    "    \"_c0\" : \"CNPJ_BASE\", \n",
    "    \"_c1\" : \"CNPJ_ORDEM\", \n",
    "    \"_c2\" : \"CNPJ_DV\", \n",
    "    \"_c3\" : \"MATRIZ_FILIAL\", \n",
    "    \"_c4\" : \"NOME_FANTASIA\", \n",
    "    \"_c5\" : \"SITUACAO_CADASTRAL\", \n",
    "    \"_c6\" : \"DATA_SITUACAO_CADASTRAL\", \n",
    "    \"_c7\" : \"MOTIVO_SITUACAO_CADASTRAL\", \n",
    "    \"_c8\" : \"CIDADE_EXTERIOR\", \n",
    "    \"_c9\" : \"PAIS\", \n",
    "    \"_c10\" : \"DATA_INICIO_ATIVIDADE\", \n",
    "    \"_c11\" : \"CNAE_PRINCIPAL\", \n",
    "    \"_c12\" : \"CNAE_SECUNDARIO\", \n",
    "    \"_c13\" : \"TIPO_LOGRADOURO\", \n",
    "    \"_c14\" : \"LOGRADOURO\", \n",
    "    \"_c15\" : \"NUMERO\", \n",
    "    \"_c16\" : \"COMPLEMENTO\", \n",
    "    \"_c17\" : \"BAIRRO\", \n",
    "    \"_c18\" : \"CEP\", \n",
    "    \"_c19\" : \"UF\", \n",
    "    \"_c20\" : \"MUNICIPIO\", \n",
    "    \"_c21\" : \"DDD1\", \n",
    "    \"_c22\" : \"TELEFONE1\", \n",
    "    \"_c23\" : \"DDD2\", \n",
    "    \"_c24\" : \"TELEFONE2\", \n",
    "    \"_c25\" : \"DDD_FAX\", \n",
    "    \"_c26\" : \"FAX\", \n",
    "    \"_c27\" : \"EMAIL\", \n",
    "    \"_c28\" : \"SITUACAO_ESPECIAL\", \n",
    "    \"_c29\" : \"DATA_SITUACAO_ESPECIAL\"\n",
    "    }\n",
    "# DEFINE O ESQUEMA\n",
    "ESQUEMA = StructType([\n",
    "    StructField(\"CNPJ_BASE\", IntegerType(), True),\n",
    "    StructField(\"CNPJ_ORDEM\", IntegerType(), True),\n",
    "    StructField(\"CNPJ_DV\", IntegerType(), True),\n",
    "    StructField(\"MATRIZ_FILIAL\", IntegerType(), True),\n",
    "    StructField(\"NOME_FANTASIA\", StringType(), True),    \n",
    "    StructField(\"SITUACAO_CADASTRAL\", IntegerType(), True),\n",
    "    StructField(\"DATA_SITUACAO_CADASTRAL\", IntegerType(), True),    \n",
    "    StructField(\"MOTIVO_SITUACAO_CADASTRAL\", IntegerType(), True),\n",
    "    StructField(\"CIDADE_EXTERIOR\", IntegerType(), True),\n",
    "    StructField(\"PAIS\", StringType(), True),\n",
    "    StructField(\"DATA_INICIO_ATIVIDADE\", IntegerType(), True),\n",
    "    StructField(\"CNAE_PRINCIPAL\", IntegerType(), True),\n",
    "    StructField(\"CNAE_SECUNDARIO\", ArrayType(IntegerType())),\n",
    "    StructField(\"TIPO_LOGRADOURO\", StringType(), True),\n",
    "    StructField(\"LOGRADOURO\", StringType(), True),\n",
    "    StructField(\"NUMERO\", IntegerType(), True),\n",
    "    StructField(\"COMPLEMENTO\", StringType(), True),\n",
    "    StructField(\"BAIRRO\", StringType(), True),\n",
    "    StructField(\"CEP\", StringType(), True),\n",
    "    StructField(\"UF\", StringType(), True),\n",
    "    StructField(\"DDD1\", IntegerType(), True),\n",
    "    StructField(\"TELEFONE1\", IntegerType(), True),\n",
    "    StructField(\"DDD2\", IntegerType(), True),\n",
    "    StructField(\"TELEFONE2\", IntegerType(), True),\n",
    "    StructField(\"DDD_FAX\", IntegerType(), True),\n",
    "    StructField(\"FAX\", IntegerType(), True),\n",
    "    StructField(\"EMAIL\", StringType(), True),\n",
    "    StructField(\"SITUACAO_ESPECIAL\", IntegerType(), True),\n",
    "    StructField(\"DATA_SITUACAO_ESPECIAL\", IntegerType(), True)\n",
    "])\n",
    "# DEFINE QUAIS COLUNAS QUEREMOS MANTER E QUAIS QUEREMOS DESCARTAR\n",
    "COLUNAS_A_MANTER = ['CNPJ_BASE', 'CNPJ_ORDEM' , 'CNPJ_DV' , 'MATRIZ_FILIAL'  , 'NOME_FANTASIA' , 'SITUACAO_CADASTRAL' ,\n",
    "              'DATA_SITUACAO_CADASTRAL'  , 'MOTIVO_SITUACAO_CADASTRAL'  , 'DATA_INICIO_ATIVIDADE'  ,\n",
    "              'CNAE_PRINCIPAL'  , 'CNAE_SECUNDARIO' , 'TIPO_LOGRADOURO'  , 'LOGRADOURO'  , 'NUMERO'  , 'COMPLEMENTO' ,\n",
    "              'BAIRRO'  , 'CEP'  , 'UF'  , 'MUNICIPIO'  , 'DDD1'  , 'TELEFONE1'  , 'DDD2'  , 'TELEFONE2'  ,\n",
    "              'DDD_FAX'  , 'FAX'  , 'EMAIL']\n",
    "# DEFINE QUAIS CNAES VAMOS TRABALHAR\n",
    "CNAES = {\n",
    "        5612100:'Serviços ambulantes de alimentação',\n",
    "        5611201:'Restaurantes e similares',\n",
    "        5611203:'Lanchonetes casas de chá de sucos e similares',\n",
    "        5611204:'Bares e outros estabelecimentos especializados em servir bebidas sem entretenimento',\n",
    "        5611205:'Bares e outros estabelecimentos especializados em servir bebidas com entretenimento',\n",
    "        4721102: 'Padaria e confeitaria com predominância de revenda'\n",
    "        }\n",
    "# DEFINE O TAMANHO DO CHUNK\n",
    "chunk_size = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------\n",
      " CNPJ_BASE                 | 7396865   \n",
      " CNPJ_ORDEM                | 1         \n",
      " CNPJ_DV                   | 68        \n",
      " MATRIZ_FILIAL             | 1         \n",
      " NOME_FANTASIA             | null      \n",
      " SITUACAO_CADASTRAL        | 8         \n",
      " DATA_SITUACAO_CADASTRAL   | 20170210  \n",
      " MOTIVO_SITUACAO_CADASTRAL | 1         \n",
      " DATA_INICIO_ATIVIDADE     | 20050518  \n",
      " CNAE_PRINCIPAL            | 1412602   \n",
      " CNAE_SECUNDARIO           | 1411801   \n",
      " TIPO_LOGRADOURO           | RUA       \n",
      " LOGRADOURO                | TUCANEIRA \n",
      " NUMERO                    | 30        \n",
      " COMPLEMENTO               | null      \n",
      " BAIRRO                    | DOS LAGOS \n",
      " CEP                       | 89136000  \n",
      " UF                        | SC        \n",
      " MUNICIPIO                 | 8297      \n",
      " DDD1                      | 47        \n",
      " TELEFONE1                 | 33851125  \n",
      " DDD2                      | 47        \n",
      " TELEFONE2                 | 33851125  \n",
      " DDD_FAX                   | 47.0      \n",
      " FAX                       | 33851125  \n",
      " EMAIL                     | null      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LENDO O DATAFRAME COM O ESQUEMA DEFINIDO\n",
    "INPUT_BASE = current_dir.replace(r\"GitHub\\ETL_CNPJ\\utilitarios\", r\"\\CNPJ_PROGRAMATICA\\ESTABELECIMENTOSCSV/\")\n",
    "ESTABELECIMENTOS = list(filter(lambda x: \".csv\" in x, os.listdir(INPUT_BASE)))\n",
    "dados = spark.read.options(delimiter = \";\", header=False, inferSchema=True).csv(f\"{INPUT_BASE}{ESTABELECIMENTOS[1]}\")\n",
    "# dados.show(1, vertical=True)\n",
    "\n",
    "# USA O MÉTODO WITHCOLUMNRENAMED() PARA RENOMEAR AS COLUNAS\n",
    "for NOME_ANTIGO, NOVO_NOME in NOVO_NOME_COLUNAS.items():\n",
    "    dados = dados.withColumnRenamed(NOME_ANTIGO, NOVO_NOME)\n",
    "\n",
    "# USA OS MÉTODOS SELECT() E DROP() PARA DESCARTAR AS COLUNAS QUE NÃO PRECISAMOS\n",
    "dados = dados.select(COLUNAS_A_MANTER)\n",
    "\n",
    "# MOSTRA O RESULTADO FINAL COM AS COLUNAS RENOMEADAS E DESCARTADAS\n",
    "dados.show(1, vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE O COD CNAE A SER BUSCADO\n",
    "codigo_cnae = 4721102\n",
    "def filtra_cnae(data, codigo_cnae):\n",
    "    \"\"\"Resumo filtra_cnae\n",
    "\n",
    "    Args:\n",
    "        data ([type]: dataframe): passa o dataframe com os dados a serem filtrados desde que tenha a coluna solicitada\n",
    "        codigo_cnae ([type] int): código cnae usado para filtrar os registros da categoria dos estabelecimentos buscados\n",
    "\n",
    "    Returns:\n",
    "        df_cnae ([type] dataframe): Retorna um dataframe com os dados filtrados;\n",
    "    \"\"\"\n",
    "    # QUERY TIPO SQL PARA FILTRAR COM BASE NO PARÂMETRO PASSADO\n",
    "    df_cnae =  data.where((f\"CNAE_PRINCIPAL == '{codigo_cnae}'\"))\n",
    "    # RETORNA A QUANTIDADE DE DADOS DESTA CONSULTA\n",
    "    print(f\"Nesta consulta temos :{df_cnae.count()} observações do cnae informado!\")\n",
    "    # SUBSTITUI OS VALORES NULOS POR STRINGS VAZIAS (SÓ FUNCIONA ATÉ AQUI PARA StructField StringTypes)\n",
    "    df_cnae = df_cnae.na.fill('')\n",
    "    # MOSTRA A NAMORADINHA DO DATA MAN\n",
    "    return df_cnae.show()\n",
    "# filtra_cnae(data=dados,codigo_cnae=codigo_cnae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` PYTHON\n",
    "for chunk in dados.limit(chunk_size).rdd.toLocalIterator():\n",
    "    print(chunk)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 20) (ntb-46 executor driver): java.io.IOException: Cannot run program \"C:\\Users\\ABRASEL NACIONAL\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\pyspark\\bin\": CreateProcess error=5, Acesso negado\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=5, Acesso negado\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 16 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Cannot run program \"C:\\Users\\ABRASEL NACIONAL\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\pyspark\\bin\": CreateProcess error=5, Acesso negado\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=5, Acesso negado\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 16 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m             df_cnae\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparque\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnappy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_BASE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescricao_cnae\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# APLICA A FUNÇÃO \"process_partition\" EM CADA PARTIÇÃO DO DATAFRAME\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mdados\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachPartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_partition\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\dataframe.py:917\u001b[0m, in \u001b[0;36mDataFrame.foreachPartition\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforeachPartition\u001b[39m(\u001b[38;5;28mself\u001b[39m, f: Callable[[Iterator[Row]], \u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    904\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies the ``f`` function to each partition of this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \n\u001b[0;32m    906\u001b[0m \u001b[38;5;124;03m    This a shorthand for ``df.rdd.foreachPartition()``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m    >>> df.foreachPartition(f)\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeachPartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:1184\u001b[0m, in \u001b[0;36mRDD.foreachPartition\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([])\n\u001b[1;32m-> 1184\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:1521\u001b[0m, in \u001b[0;36mRDD.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1513\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[0;32m   1515\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1520\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:1508\u001b[0m, in \u001b[0;36mRDD.sum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[NumberOrArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumberOrArray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m   1509\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:1336\u001b[0m, in \u001b[0;36mRDD.fold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[1;32m-> 1336\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 20) (ntb-46 executor driver): java.io.IOException: Cannot run program \"C:\\Users\\ABRASEL NACIONAL\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\pyspark\\bin\": CreateProcess error=5, Acesso negado\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=5, Acesso negado\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 16 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: Cannot run program \"C:\\Users\\ABRASEL NACIONAL\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\pyspark\\bin\": CreateProcess error=5, Acesso negado\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:157)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=5, Acesso negado\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 16 more\r\n"
     ]
    }
   ],
   "source": [
    "# APLICANDO A FUNÇÃO \"foreachPartition\" \n",
    "# ESTA FUNÇÃO PERMITE EXECUTAR UMA FUNÇÃO EM CADA PARTIÇÃO DO DATAFRAME DE FORMA PARALELA\n",
    "def process_partition(iterator):\n",
    "    # IMPORTANDO FUNÇÕES PARA CONCATENAR OS DADOS DE COLUNAS\n",
    "    from pyspark.sql.functions import concat, col \n",
    "    # CRIA O LOOP QUE VAI PARTICIONAR O DATAFRAME ORIGINAL EM PARTES\n",
    "    for dados in iterator:\n",
    "        logging.info(f'Leitura inicial: {dados.count()}')\n",
    "        # SUBSTITUI OS VALORES NULOS POR STRINGS VAZIAS (SÓ FUNCIONA ATÉ AQUI PARA StructField StringTypes)\n",
    "        dados = dados.na.fill('')\n",
    "        # QUERY TIPO SQL PARA FILTRAR COM BASE NA SITUAÇÃO CADASTRAL\n",
    "        dados = dados.where((dados[\"SITUACAO_CADASTRAL\"] == 2) | (dados[\"SITUACAO_CADASTRAL\"] == 3) | (dados[\"SITUACAO_CADASTRAL\"] == 4))\n",
    "        logging.info(f'Somente os ativos: {dados.count()}')\n",
    "        \n",
    "        # DEFINE O COD CNAE A SER BUSCADO COM BASE NO DICIONÁRIO CNAES CRIADO ANTERIORMENTE\n",
    "        for codigo_cnae, descricao_cnae in CNAES.items():\n",
    "            # QUERY TIPO SQL PARA FILTRAR COM BASE NO PARÂMETRO PASSADO\n",
    "            df_cnae =  dados.where((f\"CNAE_PRINCIPAL == '{codigo_cnae}'\"))\n",
    "            # PROCESSO DE MODELAGEM DOS DADOS \n",
    "            df_cnae = df_cnae \\\n",
    "                .withColumn(\"TELEFONE1\", concat(col(\"DDD1\").cast(\"string\"), col(\"TELEFONE1\").cast(\"string\"))) \\\n",
    "                .withColumn(\"TELEFONE2\", concat(col(\"DDD2\").cast(\"string\"), col(\"TELEFONE2\").cast(\"string\"))) \\\n",
    "                .withColumn(\"FAX\", concat(col(\"DDD_FAX\").cast(\"string\"), col(\"FAX\").cast(\"string\"))) \\\n",
    "                .drop('DDD1','DDD2','DDD_FAX', inplace=True)\n",
    "            logging.info(f\"Colunas Geradas: {df_cnae.columns}\")\n",
    "            logging.info(f\"Itens capiturados: {df_cnae.count()} Categoria dos dados: {descricao_cnae}\")\n",
    "\n",
    "            if codigo_cnae == 5612100:\n",
    "                contagem_5612100 = contagem_5612100 + df_cnae.count()\n",
    "                \n",
    "            elif codigo_cnae == 5611201:\n",
    "                contagem_5611201 = contagem_5611201 + df_cnae.count()\n",
    "\n",
    "            elif codigo_cnae == 5611203:\n",
    "                contagem_5611203 = contagem_5611203 + df_cnae.count()\n",
    "                \n",
    "            elif codigo_cnae == 5611204:\n",
    "                contagem_5611204 = contagem_5611204 + df_cnae.count()\n",
    "                \n",
    "            elif codigo_cnae == 5611205:\n",
    "                contagem_5611205 = contagem_5611205 + df_cnae.count()\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            # EXPORTA OS DADOS COM FORMATO PARQUET (POR QUESTÕES DE PROCESSAMENTO)\n",
    "            OUTPUT_BASE = current_dir.replace(\"utilitarios\", \"TESTE_BASE\")\n",
    "            df_cnae.write.format(\"parque\").mode(\"append\").option(\"compression\", \"snappy\").save(f'{OUTPUT_BASE}/{descricao_cnae}.parquet')\n",
    "\n",
    "# APLICA A FUNÇÃO \"process_partition\" EM CADA PARTIÇÃO DO DATAFRAME\n",
    "dados.repartition(1).foreachPartition(process_partition)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
